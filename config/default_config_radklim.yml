streams_directory: "./config/streams/streams_radklim/"
# streams_directory: "./config/streams/streams_anemoi_era5_split/"
data_path_anemoi: "/p/scratch/hclimrep/shared/weather_generator_data/anemoi"

embed_orientation: "channels"
embed_local_coords: True
embed_centroids_local_coords: False
embed_size_centroids: 0
embed_unembed_mode: "block"
embed_dropout_rate: 0.1

target_cell_local_prediction: True

ae_local_dim_embed: 512
ae_local_num_blocks: 2
ae_local_num_heads: 16
ae_local_dropout_rate: 0.1
ae_local_with_qk_lnorm: True

ae_local_num_queries: 1
ae_local_queries_per_cell: False
ae_adapter_num_heads: 16
ae_adapter_embed: 128
ae_adapter_with_qk_lnorm: True
ae_adapter_with_residual: True
ae_adapter_dropout_rate: 0.1

ae_global_dim_embed: 512
ae_global_num_blocks: 8
ae_global_num_heads: 32
ae_global_dropout_rate: 0.1
ae_global_with_qk_lnorm: True
ae_global_att_dense_rate: 0.2
ae_global_block_factor: 64
ae_global_mlp_hidden_factor: 2

pred_adapter_kv: False
pred_self_attention: True
pred_dyadic_dims: False
pred_mlp_adaln: True

# number of steps offset applied to first target window; if set to zero and forecast_steps=0 then
# one is training an auto-encoder
forecast_offset : 0
forecast_delta_hrs: 0
forecast_steps: 0
forecast_policy: null
forecast_freeze_model: False
forecast_att_dense_rate: 0.25
fe_num_blocks: 0
fe_num_heads: 16
fe_dropout_rate: 0.1
fe_with_qk_lnorm: True

healpix_level: 5

with_mixed_precision: True
with_flash_attention: True
compile_model: False
with_fsdp: True
attention_dtype: bf16
mlp_norm_eps: 1e-5
norm_eps: 1e-4

loss_fcts:
  -
    - "mse"
    - 1.0
loss_fcts_val:
  -
    - "mse"
    - 1.0

batch_size_per_gpu: 1
batch_size_validation_per_gpu: 1

# training mode: "forecast" or "masking" (masked token modeling)
# for "masking" to train with auto-encoder mode, forecast_offset should be 0
training_mode: "forecast"
# masking rate when training mode is "masking"; ignored in foreacast mode
masking_rate: 0.1
# sample the masking rate (with normal distribution centered at masking_rate)
masking_rate_sampling: True
# sample a subset of all target points, useful e.g. to reduce memory requirements
sampling_rate_target: 1.0
# include a masking strategy here, currently only supporting "random" and "block"
masking_strategy: "random"

num_epochs: 32
samples_per_epoch: 4096
samples_per_validation: 512
shuffle: True

lr_scaling_policy: "sqrt"
lr_start: 0.000001
lr_max: 0.0001
lr_final_decay: 0.000001
lr_final: 0.0
lr_steps_warmup: 256
lr_steps_cooldown: 512
lr_policy_warmup: "cosine"
lr_policy_decay: "linear"
lr_policy_cooldown: "linear"

grad_clip: 1.0
weight_decay: 0.1
norm_type: "LayerNorm"
nn_module: "te"

start_date: 200201010000
end_date: 202012310000
start_date_val: 202101010000
end_date_val: 202201010000
len_hrs: 6
step_hrs: 6
input_window_steps: 1

val_initial: False

loader_num_workers: 8
log_validation: 0
analysis_streams_output: ["ERA5"]

istep: 0
run_history: []

desc: ""
data_loader_rng_seed: 42
run_id: ???

# Parameters for logging/printing in the training loop
train_log:
  # The period to log metrics (in number of batch steps)
  log_interval: 20
